# LLM常见的记忆模式

## 1. 缓冲记忆

将`Human`和`Ai`生成的消息全部存储起来，每次使用时保存所有的消息列表到`Prompt`中

* 优点
  * 无损记忆
  * 实现简单，兼容性好

* 缺点

  * 由于传递消息量大，会消耗更多`token`，导致响应变慢和成本增加
  * 达到LLM的`token`上限时，太长的对话无法被记住
  * 记忆内容不是无限的。对于上下文较小的模型，记忆内容会变短

## 2. 缓存窗口记忆

只保留最近几次`Human`和`Ai`生成的消息，它基于`缓存记忆`的思想，并添加了窗口大小限制，窗口值`k`，只保留一定数量的消息

* 优点

  * 在限制使用的`token`数量表现优异
  * 对小模型友好，一般效果最佳
  * 实现简单，性能优异，所有模型都支持

* 缺点

  * 不适合遥远的记忆，因为它会忘记很久以前的记忆
  * 部分内容过长时，也会超过LLM上下文限制

## 3. 令牌缓冲记忆

只保留限定次数`Human`和`Ai`生成的消息，它基于`缓存记忆`的思想，并添加令牌数`max_tokens`，当聊天记录超过`token`时，才会遗忘记忆

* 优点

  * 可基于LLM的上下文长度限制分配记忆长度
  * 对小模型友好，一般效果最佳
  * 实现简单，性能优异，所有模型都支持

* 缺点

  * 不适合遥远的记忆

## 摘要总结记忆

除了会传递消息，还会对消息进行总结，每次只传递总结，而不是完整的消息

* 优点
  * 适合长期记忆和短期记忆(模糊记忆)
  * 减少长对话中使用`token`的数量，能记忆更多轮对话
  * 长对话时效果明显。随着对话进行，摘要方法增长速度减慢，与常规缓存内存模型相比具有优势

* 缺点
  * 会丢失细节部分
  * 对于较短的对话，可能会增加`token`消耗
  * 总结摘要部分完全依赖于中间摘要LLM的能力，需要为摘要LLM分配`token`，增加成本且未限制对话长度

## 摘要缓冲混合记忆

结合了`摘要总结记忆`和`缓冲窗口记忆`，旨在对对话进行摘要总结，同时保留最近的对话，并使用长度标记何时清除记忆

* 优点
  * 长期和短期都可记忆，长期为模糊记忆，短期为精准记忆
  * 减少长对话中使用`token`数量，能记忆更多轮对话

* 缺点
  * 长期记忆内容依然为模糊记忆
  * 总结摘要部分完全依赖于中间摘要LLM的能力，需要为摘要LLM分配`token`，增加成本且未限制对话长度

### 注意事项

`ConversationSummaryBufferMemory`会将汇总摘要的部分默认设置为`system`角色，创建系统角色信息，而其他消息则正常显示，传递的消息列表就变成：`[system, system, human, ai, human, ai, human]`。

但是部分聊天模型是不支持传递多个角色为`System`的消息，并且在`langchain_community`中集成的模型并没有对多个`System`进行集中合并封装（Chat Model 未更新同步），如果直接使用可能会出现报错。

所以在使用`ConversationSummaryMemory`这种类型的记忆组件时，需要检查对应的模型传递的`messages`的规则，以及`LangChain`是否对特定的模型封装进行了更新。

除此之外，在某些极端的场合下，例如第一条提问回复内容比较短，第二条提问内容比较长，`ConversationSummaryMemory`执行两次`Token`长度计算，如果不异步执行任务，对话速度会变得非常慢。

## 向量存储库记忆

将记忆存储在向量数据库中，并在每次调用前查询前`K`个最匹配的文档

* 优点
  * 拥有比摘要总结更强的细节，比缓冲记忆更多的内容，甚至无限长度的内容
  * 消耗的`token`比较平衡
  
* 缺点
  * 性能比其他模式相对较差，需要额外的`Embedding`和向量数据库支持
  * 记忆受检索功能影响，好的非常好，差的非常差
